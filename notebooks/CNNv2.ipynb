{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Input, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Add\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def create_corpus(tweets):\n",
    "    stop=set(stopwords.words('english'))\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(tweets):\n",
    "#         words=[word.lower() for word in nltk.casual_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "        words=[word.lower() for word in nltk.casual_tokenize(tweet)]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "corpus=create_corpus(olid['tweet_cleaned'])\n",
    "\n",
    "embedding_dict={}\n",
    "# with open('./data/glove6B/glove.6B.100d.txt','r') as f:\n",
    "with open('./data/gloveTwitter27B/glove.twitter.27B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()\n",
    "\n",
    "MAX_LEN=50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 20\n",
    "embedding_dim = 100\n",
    "\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus) #??\n",
    "sequences = tokenizer_obj.texts_to_sequences(corpus) #??\n",
    "\n",
    "tweet_pad = pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "\n",
    "word_index = tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))\n",
    "\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((num_words,embedding_dim))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec=embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec\n",
    "        \n",
    "# channel 1\n",
    "inputs1 = Input(shape=(MAX_LEN,), name = \"trainable\")\n",
    "embedding1 = Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=True)(inputs1)\n",
    "\n",
    "convs1 = []\n",
    "for fsz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                         kernel_size=fsz,\n",
    "                         padding='valid',\n",
    "                         activation='tanh',\n",
    "                         use_bias = True)(embedding1)\n",
    "\n",
    "    convs1.append(conv)\n",
    "\n",
    "# channel 2\n",
    "inputs2 = Input(shape=(MAX_LEN,), name = \"fixed\")\n",
    "embedding2 = Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)(inputs2)\n",
    "convs2 = []\n",
    "for fsz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters,\n",
    "                         kernel_size=fsz,\n",
    "                         padding='valid',\n",
    "                         activation='tanh',\n",
    "                         use_bias = True)(embedding2)\n",
    "\n",
    "    convs2.append(conv)\n",
    "    \n",
    "pool1 = GlobalMaxPooling1D() (Add()([convs1[0], convs2[0]]))\n",
    "pool2 = GlobalMaxPooling1D() (Add()([convs1[1], convs2[1]]))\n",
    "\n",
    "out = Concatenate()([pool1, pool2])\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid', name = \"output\")(out)\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "optimzer = Adam(learning_rate=1e-4)\n",
    "# compile\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimzer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, show_shapes=True, to_file='multichannel.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=tweet_pad[:tweet.shape[0]]\n",
    "# test=tweet_pad[tweet.shape[0]:]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(olid['subtask_a'])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(tweet_pad, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_valid.shape)\n",
    "\n",
    "callback = EarlyStopping(monitor='val_loss', patience=10, min_delta = 0.005)\n",
    "\n",
    "history=model.fit({\"fixed\": X_train, \"trainable\": X_train},\n",
    "                  {\"output\":y_train},\n",
    "                  batch_size=512,\n",
    "                  epochs=200,\n",
    "                  validation_data=([X_valid,X_valid],y_valid),\n",
    "                  shuffle = True,\n",
    "                  verbose=2,\n",
    "                 callbacks = [callback])\n",
    "\n",
    "proba_valid = model.predict({'fixed':X_valid,'trainable':X_valid})\n",
    "proba_train = model.predict({'fixed':X_train,'trainable':X_train})\n",
    "\n",
    "print(classification_report(y_train, np.round(proba_train)))\n",
    "print('AUC: ', roc_auc_score(y_train, proba_train))\n",
    "\n",
    "print(classification_report(y_valid, np.round(proba_valid)))\n",
    "print('AUC: ',  roc_auc_score(y_valid, proba_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(olid['subtask_a'])\n",
    "\n",
    "y_test = le.transform(olid_test['subtask_a'])\n",
    "corpus_test = create_corpus(olid_test['tweet_cleaned'])\n",
    "sequences_test = tokenizer_obj.texts_to_sequences(corpus_test) #??\n",
    "\n",
    "X_test = pad_sequences(sequences_test,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "\n",
    "callback = EarlyStopping(monitor='val_loss', patience=10, min_delta = 0.005 )\n",
    "\n",
    "history=model.fit({\"fixed\":tweet_pad,\"trainable\":tweet_pad}, \n",
    "                  {\"output\":y},\n",
    "                  batch_size=512,\n",
    "                  epochs=100,\n",
    "                  validation_data=({\"fixed\":X_test,\"trainable\":X_test}, \n",
    "                  {\"output\":y_test}),\n",
    "                  shuffle = True,\n",
    "                  verbose=2,\n",
    "                 callbacks = [callback])\n",
    "\n",
    "\n",
    "\n",
    "print(X_test.shape, y_test.shape)\n",
    "preds_test = model.predict({\"fixed\":X_test,\"trainable\":X_test})\n",
    "print(classification_report(y_test, np.round(preds_test)))\n",
    "\n",
    "print('AUC: ',  roc_auc_score(y_test, preds_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hd",
   "language": "python",
   "name": "hd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
