{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def create_corpus(tweets):\n",
    "    stop=set(stopwords.words('english'))\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(tweets):\n",
    "#         words=[word.lower() for word in nltk.casual_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "        words=[word.lower() for word in nltk.casual_tokenize(tweet)]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "corpus=create_corpus(olid['tweet_cleaned'])\n",
    "\n",
    "embedding_dict={}\n",
    "# with open('./data/glove6B/glove.6B.100d.txt','r') as f:\n",
    "with open('./data/gloveTwitter27B/glove.twitter.27B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()\n",
    "\n",
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus) #??\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus) #??\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "\n",
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))\n",
    "\n",
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec=embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec\n",
    "        \n",
    "        \n",
    "model=Sequential()\n",
    "\n",
    "embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=True)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimzer=Adam(learning_rate=1e-4)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, show_shapes=True, to_file='multichannel.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=tweet_pad[:tweet.shape[0]]\n",
    "# test=tweet_pad[tweet.shape[0]:]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(olid['subtask_a'])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(tweet_pad, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_valid.shape)\n",
    "callback = EarlyStopping(monitor='val_loss', patience=10, min_delta = 0.005 )\n",
    "\n",
    "history=model.fit(X_train,y_train,\n",
    "                  batch_size=512,\n",
    "                  epochs=200,\n",
    "                  validation_data=(X_valid,y_valid),\n",
    "                  shuffle = True,\n",
    "                  verbose=2,\n",
    "                 callbacks = [callback])\n",
    "\n",
    "preds_valid = np.round(model.predict(X_valid))\n",
    "proba_valid = model.predict_proba(X_valid)\n",
    "preds_train = np.round(model.predict(X_train))\n",
    "proba_train = model.predict_proba(X_train)\n",
    "\n",
    "print(classification_report(y_train, preds_train))\n",
    "print('AUC: ', roc_auc_score(y_train, proba_train))\n",
    "\n",
    "print(classification_report(y_valid, preds_valid))\n",
    "print('AUC: ',  roc_auc_score(y_valid, proba_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(olid['subtask_a'])\n",
    "\n",
    "y_test = le.transform(olid_test['subtask_a'])\n",
    "corpus_test = create_corpus(olid_test['tweet_cleaned'])\n",
    "sequences_test = tokenizer_obj.texts_to_sequences(corpus_test) #??\n",
    "\n",
    "X_test = pad_sequences(sequences_test,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "\n",
    "callback = EarlyStopping(monitor='val_loss', patience=10, min_delta = 0.005 )\n",
    "history=model.fit(tweet_pad,y,\n",
    "                  batch_size=512,\n",
    "                  epochs=100,\n",
    "                  validation_data=(X_test,y_test),\n",
    "                  shuffle = True,\n",
    "                  verbose=2,\n",
    "                 callbacks = [callback])\n",
    "\n",
    "\n",
    "\n",
    "print(X_test.shape, y_test.shape)\n",
    "preds_test = model.predict(X_test)\n",
    "print(classification_report(y_test, np.round(preds_test)))\n",
    "\n",
    "print('AUC: ',  roc_auc_score(y_test, preds_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hd",
   "language": "python",
   "name": "hd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
