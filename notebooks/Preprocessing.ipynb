{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /u/arsaikia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from Twitter import TwitterAccess\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "pd.options.display.max_colwidth = None\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "import random\n",
    "import numpy as np\n",
    "from multiprocessing import  Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Emojis\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "# def remove_emoji(text):\n",
    "#     emoji_pattern = re.compile(\"[\"\n",
    "#                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                            u\"\\U00002702-\\U000027B0\"\n",
    "#                            u\"\\U000024C2-\\U0001F251\"\n",
    "#                            \"]+\", flags=re.UNICODE)\n",
    "#     return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "### Hashtags\n",
    "def remove_hashtag(text):\n",
    "    hashtag=re.compile(r'#\\w+')\n",
    "    return hashtag.sub(r'',text)\n",
    "\n",
    "### Punctuations\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "### Mentions\n",
    "def remove_mentions(text):\n",
    "    mention=re.compile(r'@\\w+')\n",
    "    return mention.sub(r'',text)\n",
    "\n",
    "### URL\n",
    "def remove_URL(text):\n",
    "    URL=re.compile(r'url|&amp')\n",
    "    return URL.sub(r'',text)\n",
    "\n",
    "### Spell Checker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "def clean_tweets(df):\n",
    "\n",
    "    df['tweet_cleaned'] = df['tweet'].str.lower().apply(lambda x: remove_emoji(x))\\\n",
    "                                    .apply(lambda x: remove_hashtag(x))\\\n",
    "                                    .apply(lambda x: remove_mentions(x))\\\n",
    "                                    .apply(lambda x: remove_URL(x))\\\n",
    "                                    .apply(lambda x: remove_punct(x))\\\n",
    "                                    .str.strip()\n",
    "    #                                     .apply(lambda x: correct_spellings(x))\\\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def parallelize_cleaning(df, func, n_cores=10):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "# remove_URL(remove_pbunct(remove_mentions(remove_emoji(remove_hashtag(olid['tweet'][2]))))).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arsaikia/hate_detection\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans what their take on this is.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of shit to a volcano. ðŸ˜‚\"</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illegals to move into red states</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "0  86426   \n",
       "1  90194   \n",
       "2  16820   \n",
       "3  62688   \n",
       "4  43605   \n",
       "\n",
       "                                                                                                                                                                                    tweet  \\\n",
       "0                                                                                                                 @USER She should ask a few native Americans what their take on this is.   \n",
       "1                                                                                                                     @USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL   \n",
       "2  Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT   \n",
       "3                                                                                                                       @USER Someone should'veTaken\" this piece of shit to a volcano. ðŸ˜‚\"   \n",
       "4                                                                                                                @USER @USER Obama wanted liberals &amp; illegals to move into red states   \n",
       "\n",
       "  subtask_a subtask_b subtask_c  \n",
       "0       OFF       UNT       NaN  \n",
       "1       OFF       TIN       IND  \n",
       "2       NOT       NaN       NaN  \n",
       "3       OFF       UNT       NaN  \n",
       "4       NOT       NaN       NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = '../data/OLID/'\n",
    "PREPROCESSED = '../preprocessed/'\n",
    "\n",
    "training_data = 'olid-training-v1.0.tsv'\n",
    "\n",
    "olid = pd.read_csv(DATA_PATH + training_data, sep = '\\t')\n",
    "olid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtask_cii(col):\n",
    "    if col == 'GRP' or col == 'OTH':\n",
    "        return 'OTH'\n",
    "    else:\n",
    "        return col\n",
    "    \n",
    "def subtask_ciii(col):\n",
    "    if col == 'GRP' or col == 'OTH':\n",
    "        return col\n",
    "    else:\n",
    "        return np.NaN\n",
    "    \n",
    "olid['subtask_c_ii'] = olid['subtask_c'].apply(lambda x: subtask_cii(x))\n",
    "olid['subtask_c_iii'] = olid['subtask_c'].apply(lambda x: subtask_ciii(x))\n",
    "# olid.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "olid_clean = parallelize_cleaning(olid, clean_tweets)\n",
    "olid_clean.to_csv(PREPROCESSED + 'olid.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_test_dataset(text, labels, out, y_label):\n",
    "\n",
    "    olid_test = pd.read_csv(text, sep = '\\t')\n",
    "    olid_test_labels = pd.read_csv(labels, header = None)\n",
    "    olid_test_labels.columns = ['id', y_label]\n",
    "\n",
    "    olid_test = pd.merge(olid_test, olid_test_labels)\n",
    "    olid_test_clean = parallelize_cleaning(olid_test, clean_tweets)\n",
    "    \n",
    "    try:\n",
    "        olid_test_clean['subtask_c_ii'] = olid_test_clean['subtask_c'].apply(lambda x: subtask_cii(x))\n",
    "        olid_test_clean['subtask_c_iii'] = olid_test_clean['subtask_c'].apply(lambda x: subtask_ciii(x))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    olid_test_clean.to_csv(out, index = False)\n",
    "# olid_testa_labels\n",
    "\n",
    "create_test_dataset(DATA_PATH+'testset-levela.tsv', DATA_PATH + 'labels-levela.csv', PREPROCESSED + 'olid-levela.csv', 'subtask_a')\n",
    "create_test_dataset(DATA_PATH+'testset-levelb.tsv', DATA_PATH + 'labels-levelb.csv', PREPROCESSED + 'olid-levelb.csv', 'subtask_b')\n",
    "create_test_dataset(DATA_PATH+'testset-levelc.tsv', DATA_PATH + 'labels-levelc.csv', PREPROCESSED + 'olid-levelc.csv', 'subtask_c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hd",
   "language": "python",
   "name": "hd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
